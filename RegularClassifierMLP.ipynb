{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a basic squeme for training and test of a Multilayer Perceptron (MLP) Neural Network (NN) using Matrix-assisted laser desorption/ionization–time of flight (MALDI-TOF) mass spectrometry (MS) Data. The code is adapted to work with PyTorch Lightning.\n",
    "\n",
    "NOTE : The use of cuda depends on the GPU available, modern versions of cuda doesn't work with old GPUs and some old versions of cuda doesn't work with modern versions of pytorch and numpy. Check the dependencies before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maldi_zsl.data import SpeciesClfDataModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from maldi_zsl.models import SpeciesClassifier\n",
    "\n",
    "dm = SpeciesClfDataModule( #Is use to load the data on h5 format (data stored on a \"dictionary like\" format to ease the access)\n",
    "    \"../Data/RKIbin.h5\", # or \"RKIbin.h5\"\n",
    "    batch_size=128, # important parameter to tune, is the number of instances that are going to be considered on each train iteration\n",
    "    n_workers=2, # increase to use more CPU power\n",
    ")\n",
    "\n",
    "dm.setup(None)\n",
    "\n",
    "model = SpeciesClassifier(\n",
    "    mlp_kwargs={ #On this dictionary is the basic configuration of the NN architecture\n",
    "        \"n_inputs\" : 6000, # keep constant\n",
    "        \"n_outputs\": dm.n_species, #is different for DRIAMS and RKI\n",
    "        \"layer_dims\": [512, 256], # decides how big the network is\n",
    "        \"layer_or_batchnorm\": \"layer\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    lr=1e-4, # important to tune\n",
    "    weight_decay=0, # this you can keep constant\n",
    "    lr_decay_factor=1.00, # this you can keep constant\n",
    "    warmup_steps=250, # this you can keep constant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <HDF5 group \"/0\" (2 members)>\n",
      "0/intensity <HDF5 dataset \"intensity\": shape (11055, 6000), type \"<f8\">\n",
      "0/loc <HDF5 dataset \"loc\": shape (11055,), type \"|S158\">\n",
      "central <HDF5 dataset \"central\": shape (11055,), type \"<i8\">\n",
      "unstructured <HDF5 group \"/unstructured\" (3 members)>\n",
      "unstructured/mz <HDF5 dataset \"mz\": shape (6000,), type \"<f8\">\n",
      "unstructured/species_labels <HDF5 dataset \"species_labels\": shape (270,), type \"|S41\">\n",
      "unstructured/split <HDF5 dataset \"split\": shape (11055,), type \"|S5\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "f = h5py.File(\"../Data/RKIbin.h5\", \"r\")\n",
    "f.visititems(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'train': 8442, b'val': 1350, b'test': 1263}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {}\n",
    "for split in f[\"unstructured/split\"]:\n",
    "    if split in count: count[split]+=1\n",
    "    else: count[split] = 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "    Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for minibatch in iter(dm.train_dataloader()):\n",
    "    for i in minibatch:\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | spectrum_embedder | MLP                | 3.3 M \n",
      "1 | accuracy          | MulticlassAccuracy | 0     \n",
      "2 | top5_accuracy     | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "3.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 M     Total params\n",
      "13.099    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 66/66 [00:01<00:00, 51.20it/s, v_num=12]        \n"
     ]
    }
   ],
   "source": [
    "val_ckpt = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")\n",
    "callbacks = [val_ckpt, EarlyStopping(monitor=\"val_acc\", patience=10, mode=\"max\")]\n",
    "logger = TensorBoardLogger(\"../logs_folder\", name=\"test_run\") # Ctrl+Shift+P\n",
    "\n",
    "trainer = Trainer( #lightning module for NN training\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    strategy=\"auto\",\n",
    "    max_epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../logs_folder/test_run/version_12/checkpoints/epoch=34-step=2310.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at ../logs_folder/test_run/version_12/checkpoints/epoch=34-step=2310.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 11/11 [00:00<00:00, 123.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8970370292663574     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5547943711280823     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_top5_acc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9666666388511658     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8970370292663574    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5547943711280823    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_top5_acc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9666666388511658    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = trainer.validate(model, dm.val_dataloader(), ckpt_path=val_ckpt.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPTunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers, lr, dropout\n",
    "import os\n",
    "from torch import manual_seed as torch_manual_seed\n",
    "from numpy.random import seed as np_random_seed\n",
    "from torch.cuda import is_available as torch_cuda_is_available\n",
    "from torch.cuda import manual_seed as torch_cuda_manual_seed\n",
    "from torch.cuda import manual_seed_all as torch_cuda_manual_seed_all\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    seed_everything(seed, workers=True)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np_random_seed(seed)\n",
    "    torch_manual_seed(seed)\n",
    "    if torch_cuda_is_available():\n",
    "        torch_cuda_manual_seed(seed)\n",
    "        torch_cuda_manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "\\\\begin{table}\n",
    "\\caption{Results of the hyperparameter tunning considering the general accuracy.}\n",
    "\\centering\n",
    "\\small\n",
    "\\\\begin{tabular}{l c c c}\n",
    "\\\\toprule\n",
    "\\multicolumn{3}{c}{Parameter} & \\multicolumn{1}{c}{Result} \\\\ \\cmidrule(lr){1-3} \\cmidrule(lr){4-4}\n",
    "lr & dropout & mlp\\_hid & Accuracy\\\\\n",
    "\\midrule\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"lr\": [1e-5, 1e-4, 1e-3],\n",
    "    \"dropout\" : [0.2,0.4,0.6],\n",
    "    \"mlp_hid\" : [[256, 256],[512, 256],[512, 256, 256],[512, 512, 256]],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in config[\"lr\"]:\n",
    "    for dropout in config[\"dropout\"]:\n",
    "        for mlp_hid in config[\"mlp_hid\"]:\n",
    "            model = SpeciesClassifier(\n",
    "                mlp_kwargs={ #On this dictionary is the basic configuration of the NN architecture\n",
    "                    \"n_inputs\" : 6000, # keep constant\n",
    "                    \"n_outputs\": dm.n_species, #is different for DRIAMS and RKI\n",
    "                    \"layer_dims\": mlp_hid, # decides how big the network is\n",
    "                    \"layer_or_batchnorm\": \"layer\",\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                lr=lr, # important to tune\n",
    "                weight_decay=0, # this you can keep constant\n",
    "                lr_decay_factor=1.00, # this you can keep constant\n",
    "                warmup_steps=250, # this you can keep constant\n",
    "            )\n",
    "\n",
    "            val_ckpt = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")\n",
    "            callbacks = [val_ckpt, EarlyStopping(monitor=\"val_acc\", patience=10, mode=\"max\")]\n",
    "            logger = TensorBoardLogger(\"../logs_folder\", name=\"tune_run\") # Ctrl+Shift+P\n",
    "            trainer = Trainer( \n",
    "                accelerator=\"gpu\",\n",
    "                devices=[0],\n",
    "                strategy=\"auto\",\n",
    "                max_epochs=100,\n",
    "                callbacks=callbacks,\n",
    "                logger=logger,\n",
    "            )\n",
    "            trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\n",
    "            res = trainer.validate(model, dm.val_dataloader(), ckpt_path=val_ckpt.best_model_path)\n",
    "            acc = res[0][\"val_acc\"]\n",
    "            row=f\"\"\"\n",
    "{lr} & {dropout} & {mlp_hid} & {round(acc, 4)} \\\\\\\\\"\"\"\n",
    "            corpus+=row\n",
    "tail = \"\"\"\n",
    "\\\\bottomrule\n",
    "\\end{tabular}\n",
    "\\caption{Hyperpameter tunning of the data}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "corpus+=tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}\n",
      "\\caption{Results of the hyperparameter tunning considering the general accuracy.}\n",
      "\\centering\n",
      "\\small\n",
      "\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\multicolumn{3}{c}{Parameter} & \\multicolumn{1}{c}{Result} \\ \\cmidrule(lr){1-3} \\cmidrule(lr){4-4}\n",
      "lr & dropout & mlp\\_hid & Accuracy\\\n",
      "\\midrule\n",
      "1e-05 & 0.2 & [256, 256] & 0.8696 \\\\\n",
      "1e-05 & 0.2 & [512, 256] & 0.8607 \\\\\n",
      "1e-05 & 0.2 & [512, 256, 256] & 0.8681 \\\\\n",
      "1e-05 & 0.2 & [512, 512, 256] & 0.883 \\\\\n",
      "1e-05 & 0.4 & [256, 256] & 0.8733 \\\\\n",
      "1e-05 & 0.4 & [512, 256] & 0.8763 \\\\\n",
      "1e-05 & 0.4 & [512, 256, 256] & 0.843 \\\\\n",
      "1e-05 & 0.4 & [512, 512, 256] & 0.8622 \\\\\n",
      "1e-05 & 0.6 & [256, 256] & 0.8156 \\\\\n",
      "1e-05 & 0.6 & [512, 256] & 0.8511 \\\\\n",
      "1e-05 & 0.6 & [512, 256, 256] & 0.0867 \\\\\n",
      "1e-05 & 0.6 & [512, 512, 256] & 0.0615 \\\\\n",
      "0.0001 & 0.2 & [256, 256] & 0.9022 \\\\\n",
      "0.0001 & 0.2 & [512, 256] & 0.9052 \\\\\n",
      "0.0001 & 0.2 & [512, 256, 256] & 0.9015 \\\\\n",
      "0.0001 & 0.2 & [512, 512, 256] & 0.8807 \\\\\n",
      "0.0001 & 0.4 & [256, 256] & 0.9 \\\\\n",
      "0.0001 & 0.4 & [512, 256] & 0.8933 \\\\\n",
      "0.0001 & 0.4 & [512, 256, 256] & 0.9074 \\\\\n",
      "0.0001 & 0.4 & [512, 512, 256] & 0.9111 \\\\\n",
      "0.0001 & 0.6 & [256, 256] & 0.9015 \\\\\n",
      "0.0001 & 0.6 & [512, 256] & 0.8837 \\\\\n",
      "0.0001 & 0.6 & [512, 256, 256] & 0.883 \\\\\n",
      "0.0001 & 0.6 & [512, 512, 256] & 0.8459 \\\\\n",
      "0.001 & 0.2 & [256, 256] & 0.9074 \\\\\n",
      "0.001 & 0.2 & [512, 256] & 0.9022 \\\\\n",
      "0.001 & 0.2 & [512, 256, 256] & 0.9119 \\\\\n",
      "0.001 & 0.2 & [512, 512, 256] & 0.9104 \\\\\n",
      "0.001 & 0.4 & [256, 256] & 0.9119 \\\\\n",
      "0.001 & 0.4 & [512, 256] & 0.9163 \\\\\n",
      "0.001 & 0.4 & [512, 256, 256] & 0.9044 \\\\\n",
      "0.001 & 0.4 & [512, 512, 256] & 0.9074 \\\\\n",
      "0.001 & 0.6 & [256, 256] & 0.9141 \\\\\n",
      "0.001 & 0.6 & [512, 256] & 0.9067 \\\\\n",
      "0.001 & 0.6 & [512, 256, 256] & 0.8956 \\\\\n",
      "0.001 & 0.6 & [512, 512, 256] & 0.8985 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Hyperpameter tunning of the data}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
