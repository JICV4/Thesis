{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch import Trainer #https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from maldi_zsl_edit.data import MALDITOFDataModule\n",
    "from maldi_zsl_edit.models import ZSLClassifier\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What to tune\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"emb_dim\": tune.choice([512,1024,2048]),\n",
    "    \"lr\": tune.loguniform(1e-6, 1e-1),\n",
    "    \"dropout\": tune.choice([0.2,0.3,0.4,0.5]),\n",
    "    \"mlp_base\" : tune.choice([512, 256]),\n",
    "    \"cnn_base\" : tune.choice([64, 128]),\n",
    "    \"kernel\" : tune.choice([3,5,7]),\n",
    "    \"cnn_hid\" : tune.choice([0,64,128]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el data set y el entrenamiento debe suceder dentro de una funcion que tengo como input el set de hyperparametros\n",
    "#trainer.fit(model, dm.train_dataloader(), dm.val_dataloader()) \n",
    "\n",
    "def train_func(search_space):\n",
    "    #tune dataset\n",
    "    dm = MALDITOFDataModule( \n",
    "        \"../Data/zsl_binned_new.h5t\",\n",
    "        zsl_mode = True, \n",
    "        split_index = 0, \n",
    "        batch_size = search_space['batch_size'], # important hyperparameter?\n",
    "        n_workers = 2,\n",
    "        in_memory = True, \n",
    "        )\n",
    "    dm.setup(None)\n",
    "\n",
    "    #tune model\n",
    "    model = ZSLClassifier(\n",
    "        mlp_kwargs = { #specify the parameters to buld the MLP ()\n",
    "            'n_inputs' : 6000, #Bins of the spectra\n",
    "            'emb_dim' : search_space['emb_dim'], #This is the output of the branch\n",
    "            'layer_dims': [512, 256],\n",
    "            'layer_or_batchnorm' : \"layer\",\n",
    "            'dropout' : 0.2,\n",
    "        },\n",
    "        cnn_kwargs= { #specify the parameters to buld the CNN ()\n",
    "            'vocab_size' : 6, #Number of words, in this case is 5 as (A,T,C,G,-)\n",
    "            'emb_dim' : search_space['emb_dim'], #This is the output of the branch\n",
    "            'conv_sizes' : [search_space['cnn_base'], search_space['cnn_base']*2], #[32, 64, 128] Out chanels of the convolutions #On the nlp mode the first is an embeding dimension\n",
    "            'hidden_sizes' : [search_space['cnn_hid']], #MLP: [512, 256]. If [0] then goes directly from conv to embeding layer\n",
    "            'blocks_per_stage' : 2, #How many residual blocks are applied before the pooling\n",
    "            'kernel_size' : search_space['kernel'],\n",
    "            #Stride?\n",
    "            #Max average or non?\n",
    "            'dropout' : 0.2,\n",
    "            'nlp' : False #Move directly to the branch\n",
    "        },\n",
    "        n_classes = 160,\n",
    "        t_classes = 493,\n",
    "        lr=search_space['lr'], # important to tune\n",
    "        weight_decay=0, # this you can keep constant\n",
    "        lr_decay_factor=1.00, # this you can keep constant\n",
    "        warmup_steps=250, # this you can keep constant\n",
    "        #nlp = False #Try\n",
    "    )\n",
    "\n",
    "    #set train\n",
    "    trainer = Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses hyperband for the schedule\n",
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=3, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 05:55:23,304\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-19 05:57:04</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:37.78        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.6/62.8 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 4.0/12 CPUs, 3.0/3 GPUs (0.0/1.0 accelerator_type:T)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 10<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bbaf2_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00000_0_batch_size=16,cnn_base=64,cnn_hid=64,dropout=0.2000,emb_dim=512,kernel=5,lr=0.0195,mlp_base=512_2024-07-19_05-55-26/error.txt  </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00001_1_batch_size=32,cnn_base=64,cnn_hid=64,dropout=0.4000,emb_dim=2048,kernel=3,lr=0.0384,mlp_base=512_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00002_2_batch_size=64,cnn_base=64,cnn_hid=64,dropout=0.5000,emb_dim=2048,kernel=5,lr=0.0236,mlp_base=512_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00003</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00003_3_batch_size=32,cnn_base=128,cnn_hid=64,dropout=0.2000,emb_dim=2048,kernel=3,lr=0.0002,mlp_base=256_2024-07-19_05-55-26/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00004</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00004_4_batch_size=32,cnn_base=64,cnn_hid=64,dropout=0.4000,emb_dim=1024,kernel=7,lr=0.0409,mlp_base=256_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00005</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00005_5_batch_size=64,cnn_base=128,cnn_hid=64,dropout=0.2000,emb_dim=512,kernel=3,lr=0.0000,mlp_base=256_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00006</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00006_6_batch_size=32,cnn_base=64,cnn_hid=128,dropout=0.5000,emb_dim=512,kernel=7,lr=0.0000,mlp_base=256_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00007</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00007_7_batch_size=32,cnn_base=128,cnn_hid=0,dropout=0.4000,emb_dim=1024,kernel=5,lr=0.0111,mlp_base=256_2024-07-19_05-55-26/error.txt </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00008</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00008_8_batch_size=32,cnn_base=64,cnn_hid=0,dropout=0.5000,emb_dim=512,kernel=7,lr=0.0003,mlp_base=256_2024-07-19_05-55-26/error.txt   </td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00009</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-19_05-55-23_328915_19913/artifacts/2024-07-19_05-55-26/TorchTrainer_2024-07-19_05-55-23/driver_artifacts/TorchTrainer_bbaf2_00009_9_batch_size=32,cnn_base=64,cnn_hid=128,dropout=0.5000,emb_dim=2048,kernel=3,lr=0.0004,mlp_base=512_2024-07-19_05-55-26/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">   train_loop_config/ba\n",
       "tch_size</th><th style=\"text-align: right;\">    train_loop_config/cn\n",
       "n_base</th><th style=\"text-align: right;\">    train_loop_config/cn\n",
       "n_hid</th><th style=\"text-align: right;\">    train_loop_config/dr\n",
       "opout</th><th style=\"text-align: right;\">     train_loop_config/em\n",
       "b_dim</th><th style=\"text-align: right;\">  train_loop_config/ke\n",
       "rnel</th><th style=\"text-align: right;\">  train_loop_config/lr</th><th style=\"text-align: right;\">    train_loop_config/ml\n",
       "p_base</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bbaf2_00000</td><td>ERROR   </td><td>157.193.195.188:20964</td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">           0.0194553  </td><td style=\"text-align: right;\">512</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00001</td><td>ERROR   </td><td>157.193.195.188:21195</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">           0.0383814  </td><td style=\"text-align: right;\">512</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00002</td><td>ERROR   </td><td>157.193.195.188:21397</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">           0.0235703  </td><td style=\"text-align: right;\">512</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00003</td><td>ERROR   </td><td>157.193.195.188:21598</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">           0.000186375</td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00004</td><td>ERROR   </td><td>157.193.195.188:21798</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">           0.0409234  </td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00005</td><td>ERROR   </td><td>157.193.195.188:22000</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">           2.1204e-05 </td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00006</td><td>ERROR   </td><td>157.193.195.188:22202</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">           1.4413e-05 </td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00007</td><td>ERROR   </td><td>157.193.195.188:22403</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">           0.0110654  </td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00008</td><td>ERROR   </td><td>157.193.195.188:22604</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">           0.000325633</td><td style=\"text-align: right;\">256</td></tr>\n",
       "<tr><td>TorchTrainer_bbaf2_00009</td><td>ERROR   </td><td>157.193.195.188:22806</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">           0.000426403</td><td style=\"text-align: right;\">512</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21032)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(TorchTrainer pid=20964)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20964)\u001b[0m - (ip=157.193.195.188, pid=21032) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=20964)\u001b[0m - (ip=157.193.195.188, pid=21033) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=20964)\u001b[0m - (ip=157.193.195.188, pid=21034) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21033)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=21032)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m     \n",
      "2024-07-19 05:55:36,563\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=20964, ip=157.193.195.188, actor_id=1fb4a88bb4972e1c478e613901000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=21034, ip=157.193.195.188, actor_id=460ba8277f6cea08bf946fac01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f7dc80a3310>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=21245)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21034)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=21195)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21195)\u001b[0m - (ip=157.193.195.188, pid=21245) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21195)\u001b[0m - (ip=157.193.195.188, pid=21246) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21195)\u001b[0m - (ip=157.193.195.188, pid=21247) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21246)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21245)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m     \n",
      "2024-07-19 05:55:47,050\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=21195, ip=157.193.195.188, actor_id=806ff8d4f942fa8810a3dbd601000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=21247, ip=157.193.195.188, actor_id=ed20310e4286000513ef5b2b01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f7898abe350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=21447)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21247)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=21397)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21397)\u001b[0m - (ip=157.193.195.188, pid=21447) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21397)\u001b[0m - (ip=157.193.195.188, pid=21448) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21397)\u001b[0m - (ip=157.193.195.188, pid=21449) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21447)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21449)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m     \n",
      "2024-07-19 05:55:56,604\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=21397, ip=157.193.195.188, actor_id=52d67e007241e264ad31b44001000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=21448, ip=157.193.195.188, actor_id=3785b7822b1daf23f81bb9d501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f7a23176350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=21447)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21447)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21448)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=21598)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21598)\u001b[0m - (ip=157.193.195.188, pid=21650) world_rank=2, local_rank=2, node_rank=0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21648)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=21648)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21649)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m     \n",
      "2024-07-19 05:56:05,823\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=21598, ip=157.193.195.188, actor_id=3d7bd6e639c091ff7180228f01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=21650, ip=157.193.195.188, actor_id=d3ef5d32e6fac4150306c59b01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f5dec09b310>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21650)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21850)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(TorchTrainer pid=21798)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21798)\u001b[0m - (ip=157.193.195.188, pid=21850) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21798)\u001b[0m - (ip=157.193.195.188, pid=21851) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=21798)\u001b[0m - (ip=157.193.195.188, pid=21852) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21852)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21850)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m     \n",
      "2024-07-19 05:56:15,626\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=21798, ip=157.193.195.188, actor_id=e678200ee3caf9b1946f6b8401000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=21850, ip=157.193.195.188, actor_id=37821b7cfd541ba51a8105fe01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f979e29eb90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=22051)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21851)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=22000)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22000)\u001b[0m - (ip=157.193.195.188, pid=22051) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22000)\u001b[0m - (ip=157.193.195.188, pid=22052) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22000)\u001b[0m - (ip=157.193.195.188, pid=22053) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22051)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22052)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m     \n",
      "2024-07-19 05:56:25,823\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22000, ip=157.193.195.188, actor_id=b4e9f5bcad0d40ab035cfcbf01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22053, ip=157.193.195.188, actor_id=b185e79edabb0355725b8ebb01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f7677026a50>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=22253)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=22052)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22052)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22053)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=22202)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22202)\u001b[0m - (ip=157.193.195.188, pid=22253) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22202)\u001b[0m - (ip=157.193.195.188, pid=22254) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22202)\u001b[0m - (ip=157.193.195.188, pid=22255) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22253)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22254)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m     \n",
      "2024-07-19 05:56:34,814\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22202, ip=157.193.195.188, actor_id=9d2d3107939fad2685bab28101000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22255, ip=157.193.195.188, actor_id=6d008a62e2da9acc67b16edb01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fd5281ca350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22255)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=22403)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22403)\u001b[0m - (ip=157.193.195.188, pid=22454) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22403)\u001b[0m - (ip=157.193.195.188, pid=22455) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22403)\u001b[0m - (ip=157.193.195.188, pid=22456) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22455)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22456)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m     \n",
      "2024-07-19 05:56:44,977\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22403, ip=157.193.195.188, actor_id=9d481522de074184b92780eb01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22456, ip=157.193.195.188, actor_id=ce3959e795a64108c4fee74d01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f1e646e8dd0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22454)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=22604)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22604)\u001b[0m - (ip=157.193.195.188, pid=22656) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22604)\u001b[0m - (ip=157.193.195.188, pid=22657) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22604)\u001b[0m - (ip=157.193.195.188, pid=22658) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22657)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22658)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m     \n",
      "2024-07-19 05:56:54,875\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00008\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22604, ip=157.193.195.188, actor_id=053a6f64a4db163151c7a43b01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22657, ip=157.193.195.188, actor_id=6a527ec7a7ea58f2b94b9eb201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f79f4c27050>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22656)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=22806)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22806)\u001b[0m - (ip=157.193.195.188, pid=22857) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22806)\u001b[0m - (ip=157.193.195.188, pid=22858) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=22806)\u001b[0m - (ip=157.193.195.188, pid=22859) world_rank=2, local_rank=2, node_rank=0\n",
      "2024-07-19 05:57:04,659\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_bbaf2_00009\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22806, ip=157.193.195.188, actor_id=d1c59ac7063bb978f4e2c93701000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22858, ip=157.193.195.188, actor_id=8a729f804e20746f7fa87fa801000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fa12fe8aa90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "2024-07-19 05:57:04,669\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jorge/ray_results/TorchTrainer_2024-07-19_05-55-23' in 0.0073s.\n",
      "2024-07-19 05:57:04,676\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_bbaf2_00000, TorchTrainer_bbaf2_00001, TorchTrainer_bbaf2_00002, TorchTrainer_bbaf2_00003, TorchTrainer_bbaf2_00004, TorchTrainer_bbaf2_00005, TorchTrainer_bbaf2_00006, TorchTrainer_bbaf2_00007, TorchTrainer_bbaf2_00008, TorchTrainer_bbaf2_00009]\n",
      "2024-07-19 05:57:04,677\tINFO tune.py:1041 -- Total run time: 97.82 seconds (97.77 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "def tune_zsl_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n",
    "\n",
    "results = tune_zsl_asha(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 05:57:04,691\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22857)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=22858)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=22859)\u001b[0m     \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results\u001b[38;5;241m.\u001b[39mget_best_result(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptl/val_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
