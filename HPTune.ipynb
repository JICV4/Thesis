{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch import Trainer #https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from maldi_zsl_edit.data import MALDITOFDataModule\n",
    "from maldi_zsl_edit.models import ZSLClassifier\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What to tune\n",
    "batch_size = 16, 32 ,64\n",
    "dim_emb = 512,788,1014\n",
    "lr=1e-4,1e-6\n",
    "dropout = 0.2,0.5\n",
    "k_n = 3,5,7\n",
    "c_n = 0,64,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What to tune\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"emb_dim\": tune.choice([512,1024,2048]),\n",
    "    \"lr\": tune.loguniform(1e-6, 1e-1),\n",
    "    \"dropout\": tune.choice([0.2,0.3,0.4,0.5]),\n",
    "    \"mlp_base\" : tune.choice([512, 256]),\n",
    "    \"cnn_base\" : tune.choice([64, 128]),\n",
    "    \"kernel\" : tune.choice([3,5,7]),\n",
    "    \"cnn_hid\" : tune.choice([0,64,128]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el data set y el entrenamiento debe suceder dentro de una funcion que tengo como input el set de hyperparametros\n",
    "#trainer.fit(model, dm.train_dataloader(), dm.val_dataloader()) \n",
    "\n",
    "def train_func(search_space):\n",
    "    #tune dataset\n",
    "    dm = MALDITOFDataModule( \n",
    "        \"../Data/zsl_binned_new.h5t\",\n",
    "        zsl_mode = True, \n",
    "        split_index = 0, \n",
    "        batch_size = search_space['batch_size'], # important hyperparameter?\n",
    "        n_workers = 2,\n",
    "        in_memory = True, \n",
    "        )\n",
    "    dm.setup(None)\n",
    "\n",
    "    #tune model\n",
    "    model = ZSLClassifier(\n",
    "        mlp_kwargs = { #specify the parameters to buld the MLP ()\n",
    "            'n_inputs' : 6000, #Bins of the spectra\n",
    "            'emb_dim' : search_space['emb_dim'], #This is the output of the branch\n",
    "            'layer_dims': [512, 256],\n",
    "            'layer_or_batchnorm' : \"layer\",\n",
    "            'dropout' : 0.2,\n",
    "        },\n",
    "        cnn_kwargs= { #specify the parameters to buld the CNN ()\n",
    "            'vocab_size' : 6, #Number of words, in this case is 5 as (A,T,C,G,-)\n",
    "            'emb_dim' : search_space['emb_dim'], #This is the output of the branch\n",
    "            'conv_sizes' : [search_space['cnn_base'], search_space['cnn_base']*2], #[32, 64, 128] Out chanels of the convolutions #On the nlp mode the first is an embeding dimension\n",
    "            'hidden_sizes' : [search_space['cnn_hid']], #MLP: [512, 256]. If [0] then goes directly from conv to embeding layer\n",
    "            'blocks_per_stage' : 2, #How many residual blocks are applied before the pooling\n",
    "            'kernel_size' : search_space['kernel'],\n",
    "            #Stride?\n",
    "            #Max average or non?\n",
    "            'dropout' : 0.2,\n",
    "            'nlp' : False #Move directly to the branch\n",
    "        },\n",
    "        n_classes = 160,\n",
    "        t_classes = 493,\n",
    "        lr=search_space['lr'], # important to tune\n",
    "        weight_decay=0, # this you can keep constant\n",
    "        lr_decay_factor=1.00, # this you can keep constant\n",
    "        warmup_steps=250, # this you can keep constant\n",
    "        #nlp = False #Try\n",
    "    )\n",
    "\n",
    "    #set train\n",
    "    trainer = Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses hyperband for the schedule\n",
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=3, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 05:55:23,304\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(TorchTrainer pid=23075)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23075)\u001b[0m - (ip=157.193.195.188, pid=23126) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23075)\u001b[0m - (ip=157.193.195.188, pid=23127) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23075)\u001b[0m - (ip=157.193.195.188, pid=23128) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23126)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=23127)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m     \n",
      "2024-07-19 06:16:29,602\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23075, ip=157.193.195.188, actor_id=cc4b69757f4126154d494bd401000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23128, ip=157.193.195.188, actor_id=01b292d900c14d5dd8adaedf01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fa896006350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23128)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=23278)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23278)\u001b[0m - (ip=157.193.195.188, pid=23328) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23278)\u001b[0m - (ip=157.193.195.188, pid=23329) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23278)\u001b[0m - (ip=157.193.195.188, pid=23330) world_rank=2, local_rank=2, node_rank=0\n",
      "2024-07-19 06:16:39,621\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23278, ip=157.193.195.188, actor_id=e390176753878e21d6ea769201000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23330, ip=157.193.195.188, actor_id=ad2114aeecb12c37d8beb46501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7efee73c6350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=23329)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23330)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23328)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=23479)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23479)\u001b[0m - (ip=157.193.195.188, pid=23529) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23479)\u001b[0m - (ip=157.193.195.188, pid=23530) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23479)\u001b[0m - (ip=157.193.195.188, pid=23531) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23531)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23530)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m     \n",
      "2024-07-19 06:16:49,707\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23479, ip=157.193.195.188, actor_id=413928355a46d2244107cbd101000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23529, ip=157.193.195.188, actor_id=9cb5e282aaf37e124514ddbf01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f3483ecff90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=23731)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23529)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=23680)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23680)\u001b[0m - (ip=157.193.195.188, pid=23731) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23680)\u001b[0m - (ip=157.193.195.188, pid=23732) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23680)\u001b[0m - (ip=157.193.195.188, pid=23733) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23733)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23731)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m     \n",
      "2024-07-19 06:16:59,770\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23680, ip=157.193.195.188, actor_id=1b52737f1ddabaa42b8fd0e101000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23733, ip=157.193.195.188, actor_id=9a3aa6003e5a4743fbae4a0701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f911af06350>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=23935)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23732)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=23881)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23881)\u001b[0m - (ip=157.193.195.188, pid=23935) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23881)\u001b[0m - (ip=157.193.195.188, pid=23936) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=23881)\u001b[0m - (ip=157.193.195.188, pid=23937) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23935)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23936)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m     \n",
      "2024-07-19 06:17:09,604\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23881, ip=157.193.195.188, actor_id=17554b333b6bc81716ecb96301000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23937, ip=157.193.195.188, actor_id=4fae314e5e5a1af7b92f0c7901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f4ffc2d80d0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=24136)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=23937)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=24085)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24085)\u001b[0m - (ip=157.193.195.188, pid=24136) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24085)\u001b[0m - (ip=157.193.195.188, pid=24137) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24085)\u001b[0m - (ip=157.193.195.188, pid=24138) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=24137)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24136)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m     \n",
      "2024-07-19 06:17:19,823\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24085, ip=157.193.195.188, actor_id=569a8a36e684c6200c0c2f5401000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24138, ip=157.193.195.188, actor_id=efaff2c4e49396ff19fde6e801000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f44926c2a90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=24337)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=24136)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24136)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24138)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=24286)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24286)\u001b[0m - (ip=157.193.195.188, pid=24337) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24286)\u001b[0m - (ip=157.193.195.188, pid=24338) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24286)\u001b[0m - (ip=157.193.195.188, pid=24339) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=24338)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24337)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m     \n",
      "2024-07-19 06:17:29,824\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24286, ip=157.193.195.188, actor_id=43176e84f29f04ba37ea603601000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24339, ip=157.193.195.188, actor_id=f9924268b8988735c222747301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fc42c08ab90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayTrainWorker pid=24538)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24339)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=24487)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24487)\u001b[0m - (ip=157.193.195.188, pid=24538) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24487)\u001b[0m - (ip=157.193.195.188, pid=24539) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24487)\u001b[0m - (ip=157.193.195.188, pid=24540) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=24539)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24538)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m     \n",
      "2024-07-19 06:17:39,898\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24487, ip=157.193.195.188, actor_id=068a44ab2dd8df0f731d863e01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24538, ip=157.193.195.188, actor_id=4ecc840463852b436baf416901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fc7ec2c9510>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24540)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=24688)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24688)\u001b[0m - (ip=157.193.195.188, pid=24739) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24688)\u001b[0m - (ip=157.193.195.188, pid=24740) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24688)\u001b[0m - (ip=157.193.195.188, pid=24741) world_rank=2, local_rank=2, node_rank=0\n",
      "2024-07-19 06:17:49,617\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00008\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24688, ip=157.193.195.188, actor_id=d923a2463247e490b00622d501000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24739, ip=157.193.195.188, actor_id=90a7166c7055c9a12b5e7b5f01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f811fda0090>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19913/1874006631.py\", line 14, in train_func\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 142, in setup\n",
      "    self.train = self.dataset_type(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/thesis/Thesis/maldi_zsl_edit/data.py\", line 10, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/dataset.py\", line 43, in __init__\n",
      "    self.f = h5torch.File(file)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5torch/file.py\", line 30, in __init__\n",
      "    super().__init__(path, mode, driver=driver)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = '../Data/zsl_binned_new.h5t', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[36m(RayTrainWorker pid=24741)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24740)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24940)\u001b[0m Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[36m(RayTrainWorker pid=24740)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24740)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=24739)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=24889)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24889)\u001b[0m - (ip=157.193.195.188, pid=24940) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24889)\u001b[0m - (ip=157.193.195.188, pid=24941) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=24889)\u001b[0m - (ip=157.193.195.188, pid=24942) world_rank=2, local_rank=2, node_rank=0\n",
      "2024-07-19 06:17:59,744\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a6a4c_00009\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 2656, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24889, ip=157.193.195.188, actor_id=6ce9a8da2c49bd80cdcb4f4e01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24942, ip=157.193.195.188, actor_id=7364bead659a804ff716f50b01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fdc8c606d10>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 189, in train_fn\n",
      "    with train_func_context():\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/train/torch/config.py\", line 26, in __enter__\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 404, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "2024-07-19 06:17:59,756\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jorge/ray_results/TorchTrainer_2024-07-19_06-16-20' in 0.0085s.\n",
      "2024-07-19 06:17:59,763\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_a6a4c_00000, TorchTrainer_a6a4c_00001, TorchTrainer_a6a4c_00002, TorchTrainer_a6a4c_00003, TorchTrainer_a6a4c_00004, TorchTrainer_a6a4c_00005, TorchTrainer_a6a4c_00006, TorchTrainer_a6a4c_00007, TorchTrainer_a6a4c_00008, TorchTrainer_a6a4c_00009]\n",
      "2024-07-19 06:17:59,763\tINFO tune.py:1041 -- Total run time: 99.71 seconds (99.69 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "def tune_zsl_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n",
    "\n",
    "results = tune_zsl_asha(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 06:17:59,776\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m     Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m     PyTorch no longer supports this GPU because it is too old.\n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m     The minimum cuda capability supported by this library is 3.7.\n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24941)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=24940)\u001b[0m     \n",
      "\u001b[36m(RayTrainWorker pid=24942)\u001b[0m /home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "\u001b[36m(RayTrainWorker pid=24942)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
      "\u001b[36m(RayTrainWorker pid=24942)\u001b[0m     \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: val_acc. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results\u001b[38;5;241m.\u001b[39mget_best_result(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.11/site-packages/ray/tune/result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: val_acc. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "results.get_best_result(metric=\"val_acc\", mode=\"max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
