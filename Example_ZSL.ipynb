{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the packages\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "from lightning.pytorch import Trainer #https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping #3\n",
    "from lightning.pytorch.loggers import TensorBoardLogger #3\n",
    "from maldi_zsl_edit.data import MALDITOFDataModule #1\n",
    "from maldi_zsl_edit.models import ZSLClassifier #2\n",
    "#import h5py\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What to tune\n",
    "batch_size = 16, 32 ,64\n",
    "dim_emb = 512,788,1014\n",
    "lr=1e-4,1e-6\n",
    "dropout = 0.2,0.5\n",
    "k_n = 3,5,7\n",
    "c_n = 0,64,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data set\n",
    "dm = MALDITOFDataModule( #Personalized lightning data modules\n",
    "    \"../Data/zsl_binned_new.h5t\", #The old has problems on split\n",
    "    zsl_mode = True, # False: multi-class CLF, True: ZSL\n",
    "    split_index = 0, # independent train-val-test split numbered 0-9\n",
    "    batch_size = 16, # important hyperparameter\n",
    "    n_workers = 2, # you can leave this always if you are not CPU limited\n",
    "    in_memory = True, # you can leave this always if memory is no problem\n",
    "    )\n",
    "\n",
    "dm.setup(None)\n",
    "#batch = next(iter(dm.train_dataloader()))\n",
    "#batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now there should be a batch instance [\"seq_ohe]\", replace the batch[\"seq\"] with it in the models file\n",
    "n_species = 160 #batch['strain'].shape[0] #Number the seq considered for the train, #The batch should be 623 (463 of training and 160 of val, the rest 165 are on test)\n",
    "t_species = 463\n",
    "#To add the correct number of n_species you need to correct the sequences seen per batch\n",
    "dim_emb = 520\n",
    "model = ZSLClassifier(\n",
    "    mlp_kwargs = { #specify the parameters to buld the MLP ()\n",
    "        'n_inputs' : 6000, #Bins of the spectra\n",
    "        'emb_dim' : dim_emb, #This is the output of the branch\n",
    "        'layer_dims': [512, 256],\n",
    "        'layer_or_batchnorm' : \"layer\",\n",
    "        'dropout' : 0.2,\n",
    "    },\n",
    "    cnn_kwargs= { #specify the parameters to buld the CNN ()\n",
    "        'vocab_size' : 6, #Number of words, in this case is 5 as (A,T,C,G,-)\n",
    "        'emb_dim' : dim_emb, #This is the output of the branch\n",
    "        'conv_sizes' : [64, 128], #[32, 64, 128] Out chanels of the convolutions #On the nlp mode the first is an embeding dimension\n",
    "        'hidden_sizes' : [0], #MLP: [512, 256]. If [0] then goes directly from conv to embeding layer\n",
    "        #IMPORTANT: The models for classification have first the convolution and then a MLP, consider to also add the MLP in the model\n",
    "        #Note: The first hidden state is the embedding dim of the seq language processing and need to be optimized\n",
    "        #Note2: The last is the embedding dim for the shared space and score function\n",
    "        'blocks_per_stage' : 2, #How many residual blocks are applied before the pooling\n",
    "        'kernel_size' : 7,\n",
    "        #Stride?\n",
    "        #Max average or non?\n",
    "        'dropout' : 0.2,\n",
    "        'nlp' : False #Move directly to the branch\n",
    "    },\n",
    "    n_classes = n_species,\n",
    "    t_classes = t_species,\n",
    "    lr=1e-4, # important to tune\n",
    "    weight_decay=0, # this you can keep constant\n",
    "    lr_decay_factor=1.00, # this you can keep constant\n",
    "    warmup_steps=250, # this you can keep constant\n",
    "    #nlp = False #Try\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#Save and monitor training with tensor board\n",
    "from datetime import datetime\n",
    "timenow = datetime.now()\n",
    "strtime = timenow.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "val_ckpt = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")\n",
    "callbacks = [val_ckpt, EarlyStopping(monitor=\"val_acc\", patience=20, mode=\"max\")]\n",
    "logger = TensorBoardLogger(\"../logs_folder\", name=\"zsl_train_try2\", version=strtime) # Ctrl+Shift+P # Main folder where the training is saved and the name for the training\n",
    "\n",
    "#Training specification\n",
    "trainer = Trainer(\n",
    "    max_epochs = 100, \n",
    "    accelerator='gpu', \n",
    "    strategy='auto',\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    devices=[0]) #You can define epochs and training devices (look on documentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from check point\n",
    "sure = False\n",
    "if sure:\n",
    "    checkpoint = torch.load('../logs_folder/zsl_train_try2/2024-07-22_06-25-16/checkpoints/epoch=56-step=60078.ckpt')\n",
    "    \n",
    "    for name, param in checkpoint['state_dict'].items():\n",
    "        print(f\"Key: {name}, Shape: {param.shape}\")\n",
    "    for name, param in model.state_dict().items():\n",
    "        print(f\"Key: {name}, Shape: {param.shape}\")\n",
    "    model.state_dict().keys() == checkpoint['state_dict'].keys()\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "    Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(\n",
      "2024-07-27 18:07:12.918863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 18:07:13.818395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | spectrum_embedder | MLPEmbedding       | 3.6 M \n",
      "1 | seq_embedder      | CNNEmbedding       | 372 K \n",
      "2 | accuracy          | MulticlassAccuracy | 0     \n",
      "3 | accuracy2         | MulticlassAccuracy | 0     \n",
      "4 | top5_accuracy     | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "15.935    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\n",
      "Epoch 0 - Train loss: NA, Train accu: 0.0\n",
      "Epoch 0 - Val loss: 9.7366361618042, Val accu: 0.0\n",
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1054/1054 [10:21<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 0 - Train loss: 7.726699352264404, Train accu: 0.002490512328222394\n",
      "Epoch 0 - Val loss: 5.079351425170898, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 1: 100%|██████████| 1054/1054 [10:26<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 1 - Train loss: 6.337392330169678, Train accu: 0.0024312143214046955\n",
      "Epoch 1 - Val loss: 5.073244571685791, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 2: 100%|██████████| 1054/1054 [10:27<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 2 - Train loss: 6.252948760986328, Train accu: 0.0020754269789904356\n",
      "Epoch 2 - Val loss: 5.087851047515869, Val accu: 0.009408602491021156\n",
      "\n",
      "Epoch 3: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 3 - Train loss: 6.207500457763672, Train accu: 0.001541745732538402\n",
      "Epoch 3 - Val loss: 5.071695327758789, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 4: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 4 - Train loss: 6.1932501792907715, Train accu: 0.0019568311981856823\n",
      "Epoch 4 - Val loss: 5.073902606964111, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 5: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 5 - Train loss: 6.183481693267822, Train accu: 0.001719639403745532\n",
      "Epoch 5 - Val loss: 5.071395397186279, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 6: 100%|██████████| 1054/1054 [10:29<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 6 - Train loss: 6.175108432769775, Train accu: 0.0018382353009656072\n",
      "Epoch 6 - Val loss: 5.074913024902344, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 7: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 7 - Train loss: 6.172275066375732, Train accu: 0.0020754269789904356\n",
      "Epoch 7 - Val loss: 5.074898719787598, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 8: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 8 - Train loss: 6.1674299240112305, Train accu: 0.0026091081090271473\n",
      "Epoch 8 - Val loss: 5.078458786010742, Val accu: 0.005376344081014395\n",
      "\n",
      "Epoch 9: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 9 - Train loss: 6.152036190032959, Train accu: 0.002964895684272051\n",
      "Epoch 9 - Val loss: 4.931278228759766, Val accu: 0.02604166604578495\n",
      "\n",
      "Epoch 10: 100%|██████████| 1054/1054 [10:24<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 10 - Train loss: 5.514423847198486, Train accu: 0.014587286859750748\n",
      "Epoch 10 - Val loss: 5.168517589569092, Val accu: 0.04435483738780022\n",
      "\n",
      "Epoch 11: 100%|██████████| 1054/1054 [10:22<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 11 - Train loss: 3.817619562149048, Train accu: 0.10347485542297363\n",
      "Epoch 11 - Val loss: 5.128169536590576, Val accu: 0.13474462926387787\n",
      "\n",
      "Epoch 12: 100%|██████████| 1054/1054 [10:28<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 12 - Train loss: 2.533496856689453, Train accu: 0.28856343030929565\n",
      "Epoch 12 - Val loss: 4.7630839347839355, Val accu: 0.2271505445241928\n",
      "\n",
      "Epoch 13: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 13 - Train loss: 1.792291283607483, Train accu: 0.448213130235672\n",
      "Epoch 13 - Val loss: 5.344870567321777, Val accu: 0.2259744554758072\n",
      "\n",
      "Epoch 14: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 14 - Train loss: 1.384986400604248, Train accu: 0.5503043532371521\n",
      "Epoch 14 - Val loss: 5.999501705169678, Val accu: 0.2661290466785431\n",
      "\n",
      "Epoch 15: 100%|██████████| 1054/1054 [10:29<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 15 - Train loss: 1.067893147468567, Train accu: 0.6355154514312744\n",
      "Epoch 15 - Val loss: 6.526806831359863, Val accu: 0.27943548560142517\n",
      "\n",
      "Epoch 16: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 16 - Train loss: 0.8789428472518921, Train accu: 0.6947540640830994\n",
      "Epoch 16 - Val loss: 7.9186553955078125, Val accu: 0.25285619497299194\n",
      "\n",
      "Epoch 17: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 17 - Train loss: 0.7368922233581543, Train accu: 0.7317955493927002\n",
      "Epoch 17 - Val loss: 5.970844268798828, Val accu: 0.2800739109516144\n",
      "\n",
      "Epoch 18: 100%|██████████| 1054/1054 [10:26<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 18 - Train loss: 0.640272855758667, Train accu: 0.7687776684761047\n",
      "Epoch 18 - Val loss: 7.467745780944824, Val accu: 0.2921707034111023\n",
      "\n",
      "Epoch 19: 100%|██████████| 1054/1054 [10:25<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 19 - Train loss: 0.5441670417785645, Train accu: 0.7979522347450256\n",
      "Epoch 19 - Val loss: 9.124200820922852, Val accu: 0.28629031777381897\n",
      "\n",
      "Epoch 20: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 20 - Train loss: 0.47462064027786255, Train accu: 0.8274430632591248\n",
      "Epoch 20 - Val loss: 7.806066513061523, Val accu: 0.276780903339386\n",
      "\n",
      "Epoch 21: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 21 - Train loss: 0.42659100890159607, Train accu: 0.8471300005912781\n",
      "Epoch 21 - Val loss: 8.334189414978027, Val accu: 0.2599126398563385\n",
      "\n",
      "Epoch 22: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 22 - Train loss: 0.37510430812835693, Train accu: 0.859542965888977\n",
      "Epoch 22 - Val loss: 10.192171096801758, Val accu: 0.25134408473968506\n",
      "\n",
      "Epoch 23: 100%|██████████| 1054/1054 [10:32<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 23 - Train loss: 0.32663631439208984, Train accu: 0.8791310787200928\n",
      "Epoch 23 - Val loss: 9.555258750915527, Val accu: 0.26629704236984253\n",
      "\n",
      "Epoch 24: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 24 - Train loss: 0.28158706426620483, Train accu: 0.8956949710845947\n",
      "Epoch 24 - Val loss: 9.322287559509277, Val accu: 0.2562164068222046\n",
      "\n",
      "Epoch 25: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 25 - Train loss: 0.25724804401397705, Train accu: 0.9056569933891296\n",
      "Epoch 25 - Val loss: 9.336934089660645, Val accu: 0.25957661867141724\n",
      "\n",
      "Epoch 26: 100%|██████████| 1054/1054 [10:25<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 26 - Train loss: 0.23550768196582794, Train accu: 0.9121204614639282\n",
      "Epoch 26 - Val loss: 8.889808654785156, Val accu: 0.2617943584918976\n",
      "\n",
      "Epoch 27: 100%|██████████| 1054/1054 [10:24<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 27 - Train loss: 0.1952337920665741, Train accu: 0.9258973598480225\n",
      "Epoch 27 - Val loss: 9.410246849060059, Val accu: 0.27940186858177185\n",
      "\n",
      "Epoch 28: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 28 - Train loss: 0.19774869084358215, Train accu: 0.9257787466049194\n",
      "Epoch 28 - Val loss: 10.291045188903809, Val accu: 0.24996641278266907\n",
      "\n",
      "Epoch 29: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 29 - Train loss: 0.17610514163970947, Train accu: 0.9344757795333862\n",
      "Epoch 29 - Val loss: 8.568325996398926, Val accu: 0.2686491906642914\n",
      "\n",
      "Epoch 30: 100%|██████████| 1054/1054 [10:32<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 30 - Train loss: 0.16321530938148499, Train accu: 0.9362744688987732\n",
      "Epoch 30 - Val loss: 8.391498565673828, Val accu: 0.2840389609336853\n",
      "\n",
      "Epoch 31: 100%|██████████| 1054/1054 [10:32<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 31 - Train loss: 0.15771152079105377, Train accu: 0.9385673403739929\n",
      "Epoch 31 - Val loss: 8.446396827697754, Val accu: 0.2803763449192047\n",
      "\n",
      "Epoch 32: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 32 - Train loss: 0.15878604352474213, Train accu: 0.9396347403526306\n",
      "Epoch 32 - Val loss: 9.38868522644043, Val accu: 0.2827621102333069\n",
      "\n",
      "Epoch 33: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 33 - Train loss: 0.14201132953166962, Train accu: 0.9445762038230896\n",
      "Epoch 33 - Val loss: 12.599315643310547, Val accu: 0.27268144488334656\n",
      "\n",
      "Epoch 34: 100%|██████████| 1054/1054 [10:24<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 34 - Train loss: 0.14124494791030884, Train accu: 0.9474620223045349\n",
      "Epoch 34 - Val loss: 13.111355781555176, Val accu: 0.30174732208251953\n",
      "\n",
      "Epoch 35: 100%|██████████| 1054/1054 [10:25<00:00,  1.69it/s, v_num=7-12]\n",
      "Epoch 35 - Train loss: 0.13697481155395508, Train accu: 0.9463353753089905\n",
      "Epoch 35 - Val loss: 10.110434532165527, Val accu: 0.3015457093715668\n",
      "\n",
      "Epoch 36: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 36 - Train loss: 0.13250218331813812, Train accu: 0.9467504620552063\n",
      "Epoch 36 - Val loss: 9.712800979614258, Val accu: 0.26377686858177185\n",
      "\n",
      "Epoch 37: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 37 - Train loss: 0.11955495178699493, Train accu: 0.9530755877494812\n",
      "Epoch 37 - Val loss: 11.67224407196045, Val accu: 0.27570563554763794\n",
      "\n",
      "Epoch 38: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 38 - Train loss: 0.11939528584480286, Train accu: 0.9516919255256653\n",
      "Epoch 38 - Val loss: 9.074135780334473, Val accu: 0.2676747143268585\n",
      "\n",
      "Epoch 39: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 39 - Train loss: 0.12290669232606888, Train accu: 0.9510199427604675\n",
      "Epoch 39 - Val loss: 10.172791481018066, Val accu: 0.31586021184921265\n",
      "\n",
      "Epoch 40: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 40 - Train loss: 0.11884190142154694, Train accu: 0.9525023698806763\n",
      "Epoch 40 - Val loss: 10.438821792602539, Val accu: 0.2700940668582916\n",
      "\n",
      "Epoch 41: 100%|██████████| 1054/1054 [10:25<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 41 - Train loss: 0.11678217351436615, Train accu: 0.9546963572502136\n",
      "Epoch 41 - Val loss: 10.674332618713379, Val accu: 0.27268144488334656\n",
      "\n",
      "Epoch 42: 100%|██████████| 1054/1054 [10:25<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 42 - Train loss: 0.1115729957818985, Train accu: 0.9568904042243958\n",
      "Epoch 42 - Val loss: 11.802008628845215, Val accu: 0.2782258093357086\n",
      "\n",
      "Epoch 43: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 43 - Train loss: 0.09647084772586823, Train accu: 0.9611005783081055\n",
      "Epoch 43 - Val loss: 9.846064567565918, Val accu: 0.3036290109157562\n",
      "\n",
      "Epoch 44: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 44 - Train loss: 0.09969320893287659, Train accu: 0.960230827331543\n",
      "Epoch 44 - Val loss: 11.89759349822998, Val accu: 0.2745295763015747\n",
      "\n",
      "Epoch 45: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 45 - Train loss: 0.08525864779949188, Train accu: 0.9646584391593933\n",
      "Epoch 45 - Val loss: 13.802886962890625, Val accu: 0.27520161867141724\n",
      "\n",
      "Epoch 46: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 46 - Train loss: 0.10896378755569458, Train accu: 0.9567717909812927\n",
      "Epoch 46 - Val loss: 12.813488006591797, Val accu: 0.26176074147224426\n",
      "\n",
      "Epoch 47: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 47 - Train loss: 0.10534116625785828, Train accu: 0.9586693644523621\n",
      "Epoch 47 - Val loss: 9.758010864257812, Val accu: 0.2496303915977478\n",
      "\n",
      "Epoch 48: 100%|██████████| 1054/1054 [10:26<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 48 - Train loss: 0.09013848751783371, Train accu: 0.9618714451789856\n",
      "Epoch 48 - Val loss: 12.134147644042969, Val accu: 0.2666330635547638\n",
      "\n",
      "Epoch 49: 100%|██████████| 1054/1054 [10:26<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 49 - Train loss: 0.08840060234069824, Train accu: 0.964302659034729\n",
      "Epoch 49 - Val loss: 13.46413516998291, Val accu: 0.2657257914543152\n",
      "\n",
      "Epoch 50: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 50 - Train loss: 0.09029105305671692, Train accu: 0.9615749716758728\n",
      "Epoch 50 - Val loss: 12.558014869689941, Val accu: 0.2936827838420868\n",
      "\n",
      "Epoch 51: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 51 - Train loss: 0.09036432951688766, Train accu: 0.9643619656562805\n",
      "Epoch 51 - Val loss: 14.132990837097168, Val accu: 0.26367607712745667\n",
      "\n",
      "Epoch 52: 100%|██████████| 1054/1054 [10:31<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 52 - Train loss: 0.09197753667831421, Train accu: 0.9630178213119507\n",
      "Epoch 52 - Val loss: 9.333610534667969, Val accu: 0.25184813141822815\n",
      "\n",
      "Epoch 53: 100%|██████████| 1054/1054 [10:29<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 53 - Train loss: 0.0905349999666214, Train accu: 0.9622865319252014\n",
      "Epoch 53 - Val loss: 10.947343826293945, Val accu: 0.3098454177379608\n",
      "\n",
      "Epoch 54: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 54 - Train loss: 0.08187633007764816, Train accu: 0.964421272277832\n",
      "Epoch 54 - Val loss: 10.680734634399414, Val accu: 0.29553091526031494\n",
      "\n",
      "Epoch 55: 100%|██████████| 1054/1054 [10:28<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 55 - Train loss: 0.07654937356710434, Train accu: 0.9665559530258179\n",
      "Epoch 55 - Val loss: 10.552538871765137, Val accu: 0.2906585931777954\n",
      "\n",
      "Epoch 56: 100%|██████████| 1054/1054 [10:25<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 56 - Train loss: 0.07780260592699051, Train accu: 0.966140866279602\n",
      "Epoch 56 - Val loss: 11.454801559448242, Val accu: 0.28881049156188965\n",
      "\n",
      "Epoch 57: 100%|██████████| 1054/1054 [10:28<00:00,  1.68it/s, v_num=7-12]\n",
      "Epoch 57 - Train loss: 0.08269869536161423, Train accu: 0.9662001729011536\n",
      "Epoch 57 - Val loss: 13.509844779968262, Val accu: 0.2938508093357086\n",
      "\n",
      "Epoch 58: 100%|██████████| 1054/1054 [10:32<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 58 - Train loss: 0.07660460472106934, Train accu: 0.9665559530258179\n",
      "Epoch 58 - Val loss: 12.341248512268066, Val accu: 0.29553091526031494\n",
      "\n",
      "Epoch 59: 100%|██████████| 1054/1054 [10:30<00:00,  1.67it/s, v_num=7-12]\n",
      "Epoch 59 - Train loss: 0.09175200760364532, Train accu: 0.9634724855422974\n",
      "Epoch 59 - Val loss: 11.353184700012207, Val accu: 0.25678762793540955\n",
      "\n",
      "Epoch 59: 100%|██████████| 1054/1054 [11:03<00:00,  1.59it/s, v_num=7-12]\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "trainer.fit(model, dm.train_dataloader(), dm.val_dataloader()) #Important: normally you can use only dm, but here we specify as the dim of a are different for train and val \n",
    "#Note: The model object specify what is considered an input values and what is considered an input/output value during the training on the training step method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ZSLmodel2024-07-27_18-07-12.pth\n"
     ]
    }
   ],
   "source": [
    "#timenow = datetime.now()\n",
    "#traintime = timenow.strftime(\"%Y-%m-%d_%H-%M-%S\") - strtime\n",
    "#print(f\"The model lasted {traintime} to train\")\n",
    "\n",
    "#Save the model also at the end of the training\n",
    "from torch import save as torch_save\n",
    "sure = True\n",
    "if sure:\n",
    "    torch_save(model, f'../SavedModels/ZSLmodel{strtime}.pth')\n",
    "    print(f\"Saved as ZSLmodel{strtime}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "from torch import load as torch_load\n",
    "sure = True\n",
    "if sure:\n",
    "    model = torch_load('../SavedModels/ZSLmodel2024-07-22_06-25-16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6000])\n",
      "torch.Size([160, 6, 2236])\n"
     ]
    }
   ],
   "source": [
    "minibatch = next(iter(dm.val_dataloader()))\n",
    "print(minibatch[\"intensity\"].shape)\n",
    "print(minibatch[\"seq_ohe\"].shape) #The batch should be 623 (463 of training and 160 of val, the rest 165 are on test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Getting predictions ---\n",
      "Working with validation set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/jorge/anaconda3/envs/Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:190: UserWarning: \n",
      "    Found GPU1 NVIDIA Tesla K40c which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multi level evaluation ---\n",
      "For Family there are 33 different labels\n",
      "For Genus there are 52 different labels\n",
      "For Species there are 152 different labels\n",
      "For Strain there are 160 different labels\n",
      "\n",
      "--- Calculating Accuracy ---\n",
      "For the level Family the accu score is: 0.6352465748786926\n",
      "For the level Genus the accu score is: 0.5973742008209229\n",
      "For the level Species the accu score is: 0.3475845754146576\n",
      "For the level Strain the accu score is: 0.3218313455581665\n",
      "\n",
      "--- Calculating F1 scores ---\n",
      "For the level Family the F1 score is: 0.3254104256629944\n",
      "For the level Genus the F1 score is: 0.3524334728717804\n",
      "For the level Species the F1 score is: 0.26579439640045166\n",
      "For the level Strain the F1 score is: 0.24963624775409698\n"
     ]
    }
   ],
   "source": [
    "from maldi_zsl_edit.utils import ZSL_levels_metrics\n",
    "\n",
    "#For validation\n",
    "data_path = \"../Data/zsl_binned_new.h5t\"\n",
    "#data_set = dm.val_dataloader()\n",
    "levels = [\"Family\", \"Genus\", \"Species\", \"Strain\"]\n",
    "a,b = ZSL_levels_metrics(data_path,model,levels) #Consider to add a train or val mode and a high batch size inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Getting predictions ---\n",
      "Working with train set\n",
      "\n",
      "\n",
      "--- Multi level evaluation ---\n",
      "For Family there are 77 different labels\n",
      "For Genus there are 141 different labels\n",
      "For Species there are 451 different labels\n",
      "For Strain there are 463 different labels\n",
      "\n",
      "--- Calculating Accuracy ---\n",
      "For the level Family the accu score is: 0.9964399933815002\n",
      "For the level Genus the accu score is: 0.9960840344429016\n",
      "For the level Species the accu score is: 0.9781060814857483\n",
      "For the level Strain the accu score is: 0.9660021066665649\n",
      "\n",
      "--- Calculating F1 scores ---\n",
      "For the level Family the F1 score is: 0.9900447726249695\n",
      "For the level Genus the F1 score is: 0.9922187328338623\n",
      "For the level Species the F1 score is: 0.978141188621521\n",
      "For the level Strain the F1 score is: 0.9641870260238647\n"
     ]
    }
   ],
   "source": [
    "#For train\n",
    "c,d = ZSL_levels_metrics(data_path,model,levels,\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Info of the data set\n",
    "import h5py\n",
    "h5spectra = h5py.File(\"../Data/zsl_binned_new.h5t\", \"r\") # The old version has problems on the split\n",
    "#h5spectra.visititems(print) #See the data\n",
    "#There are train: 463, val :160 and test: 165 \n",
    "#torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the predictions to look their individual accuracies\n",
    "import numpy as np\n",
    "ev_species = {}\n",
    "ev_species[b'train'] = []\n",
    "ev_species[b'val_geni'] = []\n",
    "ev_species[b'val_spec'] = []\n",
    "ev_species[b'val_strain'] = []\n",
    "study = [b'train',b'val_geni',b'val_spec',b'val_strain']\n",
    "i = 0\n",
    "for label in h5spectra[\"0\"][\"split_0\"]:\n",
    "    if label in study:\n",
    "        a = h5spectra[\"central\"][i]\n",
    "        b = np.where(a == True)[0][0]\n",
    "        ev_species[label].append(b)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5941, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-26.6596, -24.7468, -27.5153,  ..., -40.4013, -24.6716, -27.3861],\n",
       "        [-44.8781, -36.4867, -35.9950,  ..., -56.0938, -38.7423, -45.9434],\n",
       "        [-12.2353, -15.5614, -13.2962,  ..., -22.6843, -12.1105, -21.2666],\n",
       "        ...,\n",
       "        [-28.7306, -31.2096, -27.5706,  ..., -46.6892, -49.7223, -28.9330],\n",
       "        [-52.5257, -53.3985, -47.9212,  ..., -75.5992, -76.6577, -52.0940],\n",
       "        [-34.3973, -34.1811, -26.4999,  ..., -49.5889, -46.2922, -28.2480]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Manual calculation of the predictions\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "y_pred = torch.empty((0,160)) #the second is the number of species #Change to 788 or 463 for val vs train\n",
    "y_real= []\n",
    "with torch.no_grad():\n",
    "    for minibatch in iter(dm.val_dataloader()): #On the split said if train, val, etc, \n",
    "        y_hat = model(minibatch)\n",
    "        y_pred = torch.cat((y_pred,y_hat),dim=0)\n",
    "        y_real+= list(minibatch['strain'])\n",
    "print(y_pred.shape) #(batch size, total possible species)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels for Multilevel evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ind = torch.argmax(y_pred, axis=1)\n",
    "real_ind = y_real\n",
    "levels = [\"Family\", \"Genus\", \"Species\", \"Strain\"]\n",
    "granularity_lvl = len(levels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filos = minibatch[\"seq_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the multilevel predictions, consider how the data is encoded (genus, species, strain)\n",
    "ml_real = []\n",
    "ml_pred = []\n",
    "for i in range(len(y_real)):\n",
    "  #for real:\n",
    "  s_real = filos[real_ind[i]].split(\";\")\n",
    "  ml_real.append(s_real)\n",
    "  #for pred:\n",
    "  s_pred = filos[pred_ind[i]].split(\";\")\n",
    "  ml_pred.append(s_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get them on the right format\n",
    "import numpy as np\n",
    "ml_real = np.array(ml_real).T\n",
    "ml_pred = np.array(ml_pred).T\n",
    "#List for better iteratation\n",
    "ml_reals = ml_real.tolist()\n",
    "ml_preds = ml_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the possible multilevel labels\n",
    "ml_level = []\n",
    "for i in range(len(filos)):\n",
    "  s_level = filos[i].split(\";\")\n",
    "  ml_level.append(s_level)\n",
    "ml_level = np.array(ml_level).T\n",
    "ml_levels = ml_level.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Family there are 33 different labels\n",
      "For Genus there are 52 different labels\n",
      "For Species there are 152 different labels\n",
      "For Strain there are 160 different labels\n"
     ]
    }
   ],
   "source": [
    "#Total number of labels\n",
    "for i in range(granularity_lvl):\n",
    "    n = len(list(set(ml_levels[i])))\n",
    "    print(f\"For {levels[i]} there are {n} different labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score #There is also a torch version, consider it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "#Create a accuracy evaluator\n",
    "def accu_score(y_true, y_pred, level_lab):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true_encoded = label_encoder.fit_transform(level_lab)\n",
    "    y_true_encoded = label_encoder.transform(y_true)\n",
    "    y_pred_encoded = label_encoder.transform(y_pred)\n",
    "\n",
    "    #Using sklearn\n",
    "    #accu = accuracy_score(y_true_encoded, y_pred_encoded, normalize=True) #The normalize True = number of correct predictions, False = fraction of correct predictions\n",
    "    \n",
    "    #Using torch\n",
    "    accu = Accuracy(task=\"multiclass\", num_classes=len(set(level_lab))) \n",
    "    accu = accu(torch.tensor(y_pred_encoded), torch.tensor(y_true_encoded))\n",
    "    \n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run accu for each level of complexity\n",
    "accu_levels = []\n",
    "for level in range(granularity_lvl ):\n",
    "  accu_levels.append(accu_score(ml_reals[level], ml_preds[level], ml_levels[level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the level Family the accu score is: 0.6105032563209534\n",
      "For the level Genus the accu score is: 0.5556303858757019\n",
      "For the level Species the accu score is: 0.2676317095756531\n",
      "For the level Strain the accu score is: 0.2420467883348465\n"
     ]
    }
   ],
   "source": [
    "# see the results\n",
    "for i in range(granularity_lvl ):\n",
    "  print(f\"For the level {levels[i]} the accu score is: {accu_levels[i]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "#Create an F1 evaluator\n",
    "def f1_macro_score(y_true, y_pred, level_lab): #micro average is basically accuracy\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true_encoded = label_encoder.fit_transform(level_lab)\n",
    "    y_true_encoded = label_encoder.transform(y_true)\n",
    "    y_pred_encoded = label_encoder.transform(y_pred)\n",
    "\n",
    "    #Using sklearn\n",
    "    #f1_scores = f1_score(y_true_encoded, y_pred_encoded, average=None)\n",
    "    #macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    #Using torch\n",
    "    macro_f1 = F1Score(task=\"multiclass\", num_classes=len(set(level_lab)), average='macro') \n",
    "    macro_f1 = macro_f1(torch.tensor(y_pred_encoded), torch.tensor(y_true_encoded))\n",
    "\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run f1_macro_score for each level of complexity\n",
    "F1_levels = []\n",
    "for level in range(granularity_lvl ):\n",
    "  F1_levels.append(f1_macro_score(ml_reals[level], ml_preds[level], ml_levels[level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the level Family the F1 score is: 0.2802780866622925\n",
      "For the level Genus the F1 score is: 0.2948206663131714\n",
      "For the level Species the F1 score is: 0.19715124368667603\n",
      "For the level Strain the F1 score is: 0.1872057020664215\n"
     ]
    }
   ],
   "source": [
    "# see the results\n",
    "for i in range(granularity_lvl ):\n",
    "  print(f\"For the level {levels[i]} the F1 score is: {F1_levels[i]}\") #The predictions are no the same as the output, maybe F1 is not used there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
